{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdfa234-2d67-4833-ad94-98aba2dc32eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential, clone_and_build_model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from transformers import pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d1228b6-a555-4839-b653-281ad73f8ecd",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Our main research question is: **Can we use the text of user reviews to predict the star ratings for reviews of businesses in Philadelphia?**\n",
    "\n",
    "To answer this question, we'll try to build a model that takes a Yelp review and predicts the star rating. Then, to see whether star ratings accurately match sentiment analysis, we will compare both the true star rating labels and our predicted star ratings to the sentiment analysis results of a pretrained model from Hugging Face."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b329700-36d1-4c5e-844b-5114029a0c24",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "## Loading Data\n",
    "The initial dataset was quite large (even when we limited our focus to only Philadelphia). Thus, we had to split the `.csv` files up, which is why we have this slightly clunky import code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45457978-614e-4090-8b38-5d7ff15f57fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in business data\n",
    "business = pd.read_json('data/yelp_academic_dataset_business.json', lines=True)\n",
    "# keep only those businesses in the city with the most observations (Philadelphia)\n",
    "# and only businesses which are restaurants\n",
    "max_city = business['city'].value_counts().idxmax()\n",
    "business = business.loc[(business['city'] == max_city) & (business['categories']).str.contains('Restaurants')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49843aad-8f2c-4103-bd99-3b49d7ed8516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in review data from separate csv's located in data folder\n",
    "review = []\n",
    "for i in range(1, 8):\n",
    "    review.append(pd.read_csv(f'data/review{i}.csv'))\n",
    "review = pd.concat(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4469d2-d66b-457b-80ce-1b02ae256900",
   "metadata": {},
   "source": [
    "### Inspecting `business`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87ca81-feca-4781-b9f5-6ed872a5b90d",
   "metadata": {},
   "source": [
    "We don't really have to worry about missing values for our `business` dataset. The only featurse with any missing values are `hours` and `attributes` (itself a dictionary of binary categories which describe the business, such as \"nice ambiance\" or \"good views\") which we are not planning on incorporating into our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd66304-a9ef-4a8a-a6fb-16006341c71c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "business.isnull().sum()/len(business.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818799c-dc37-4ba4-9aa9-589301e1e001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "business.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3bc06-7e71-4b5d-9ad2-49264f5275a3",
   "metadata": {},
   "source": [
    "It seems that four- and five-star reviews are more common than lower ratings, which later motivates our decision to consider four and five star reviews to be positive, while considering one to three star reviews to be negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8d189-1864-4dc9-9394-537b4d696f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(business['stars'], bins=np.arange(0.75, 5.75, 0.5), edgecolor=\"white\")\n",
    "plt.xlabel(\"Stars\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Frequency of Star Values for Restaurants\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d74d067",
   "metadata": {},
   "source": [
    "The distribution of review counts in the strip plot below closely mirrors the histogram showing frequency of star values. For example, four star reviews appear the most in the dataset, and restaurants with an average of four stars often have the most reviews attributed to them. On the other hand, not many restaurants have an average of one or five stars, and those with low ratings don’t have many reviews. This suggests that when more reviews are left for a restaurant, the average rating starts to converge toward four stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75b2f8-2a0c-421f-95aa-5b18700f5121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.stripplot(data=business, x='stars', y='review_count', alpha = 0.2, jitter=0.3)\n",
    "plt.ylim(-10, 2000)\n",
    "plt.xlabel(\"Average Star Rating\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.title(\"Review Counts for Restaurants across Average Star Rating\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04972ae8-26d7-4c25-bb31-0a2cce99f9f8",
   "metadata": {},
   "source": [
    "### Inspecting `reviews`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6158c-5c1f-4293-8e47-0a3d3932745b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review.isnull().sum()/len(review.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2161431-a3bd-400c-87ad-e1ac994de7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8c154ad",
   "metadata": {},
   "source": [
    "The distribution of stars for reviews shows a different pattern than the distribution of stars for restaurants. The plurality of reviews are five stars and there are more one-star reviews than two-star reviews, but when the reviews are averaged for each restaurant, there is a regression toward the mean effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ecf531-5e4a-47d0-920f-aaeaa9bf3c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(review['stars'], bins=np.arange(0.5, 6.5, 1), edgecolor=\"white\")\n",
    "plt.xlabel(\"Stars\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Frequency of Star Values for Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0f9ff0c",
   "metadata": {},
   "source": [
    "Most reviews are short and the number of reviews of a given length generally decreases as the length increases, but there are more 4500-5000 character reviews than there are 4000-4500 character reviews. The boxplots indicate that reviews for businesses with two or three stars are slightly longer while reviews for businesses with five stars tend to be shorter, but generally reviews of all star ratings follow a very similar distribution. Additionally, for all star ratings, there are many outliers with long review lengths. Because most of the reviews are shorter, when pre-processing the reviews text we should be able to truncate longer reviews without too much negative impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47fbdf-918f-452e-b26f-1cb4c0589473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(review['text'].str.len(), edgecolor=\"white\")\n",
    "plt.xlabel('Review Length (Characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')\n",
    "plt.title('Histogram of Review Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf433520-9dbd-4812-8ca4-2c4ea8fe1ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=review['stars'], y=review['text'].str.len())\n",
    "plt.xlabel(\"Stars\")\n",
    "plt.ylabel(\"Review Length (Characters)\")\n",
    "plt.title(\"Distribution of Review Length by Star Rating\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ee774c6-1efd-43e7-a4a7-dd43a2282a81",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "In order to prepare `reviews`, we simply use `text` as our input variable and `stars` as our output variable. We then save the arrays for future use. However, in order to feed `text` into a neural network, we first had to tokenize it -- this process took a long time, so the code is commented out (so that it is not accidentally run). We utilized `wordpunct_tokenize()` in order to pre-process the text. Additionally, because we chose to pad the sequences, we started counting our word to index dictionary at 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0826908-7bcc-4374-a85d-72b1763f112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    return wordpunct_tokenize(string.lower())\n",
    "\n",
    "review['text'] = review['text'].apply(clean)\n",
    "words = review['text'].apply(set)\n",
    "words = set().union(*words)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e98fcc-746a-470d-80af-faa26cbc1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "f = open(\"data/word2idx.pkl\", \"wb\")\n",
    "pickle.dump(word2idx, f)\n",
    "f.close()\n",
    "X = review['text'].apply(lambda r: [word2idx[w] for w in r])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e04ff7-e7de-435e-86e4-d4ab603a5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = max([len(i) for i in X])\n",
    "X = pad_sequences(X, max_words)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681275d-db69-4a03-ba43-01a37214fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = review['stars'].to_numpy()\n",
    "np.savez_compressed('data/reviews.npz', X=X, y=y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0787ac7a-0e2c-4d54-a303-0a129ae01e41",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "\n",
    "First, we load in the data from `reviews.npz` and use `tensorflow` `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633465a-49f7-4ff2-999d-504505de1e65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for quick loading\n",
    "reviews = np.load('data/reviews.npz')\n",
    "X, y = reviews['X'], reviews['y'] - 1\n",
    "with open('data/word2idx.pkl', 'rb') as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "## train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=109)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=109)\n",
    "\n",
    "## check shapes\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf07d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data pipeline\n",
    "batch_size = 1024\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_data = tf.data.Dataset.from_tensor_slices((X_val, y_val)).shuffle(len(X_val)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(len(X_test)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d52f5c9e",
   "metadata": {},
   "source": [
    "## Naive Feed Forward Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c9994a3",
   "metadata": {},
   "source": [
    "This is the most naive approach to the problem. Given the complexity of the text, we didn't expect a feed forward neural network to perform very well, specifically because it would have issues remembering context (given that it has no recurrent relationships between different nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff1 = Sequential(name='Naive_FFNN')\n",
    "ff1.add(Dense(64, activation='relu',input_dim=X_train.shape[1]))\n",
    "ff1.add(Dense(5, activation='softmax'))\n",
    "ff1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "ff1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "ff1.fit(train_data, validation_data=val_data, epochs=epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b1c5fce",
   "metadata": {},
   "source": [
    "As expected, this model did not perform very well. While we only trained for 10 epochs, we could see that the model accuracy on the validation set would not get much higher than 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1826c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 11), ff1.history.history['loss'], label='train')\n",
    "plt.plot(range(1, 11), ff1.history.history['val_loss'], label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Categorical Cross Entropy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c823963",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 11), ff1.history.history['accuracy'], label='train')\n",
    "plt.plot(range(1, 11), ff1.history.history['val_accuracy'], label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aaf24a69",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff1.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40749177",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(ff1.predict(X_test, batch_size = 1024), axis = -1).flatten() \n",
    "\n",
    "# Creating a confusion matrix, which compares the y_test and y_pred\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "cm_df = pd.DataFrame(cm, index = [1, 2, 3, 4, 5], columns = [1, 2, 3, 4, 5]) \n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(5,4)) \n",
    "sns.heatmap(cm_df, annot=True) \n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values') \n",
    "plt.xlabel('Predicted Values') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb490ed4",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b8fa782",
   "metadata": {},
   "source": [
    "After training our baseline model, a Feed Forward Neural Network, we observed that it predicted 5 stars for almost every review because there was an over-concentration of 5-star reviews in the dataset from Yelp. This shows that the class imbalance we identified in EDA has a large negative impact on model performance. Therefore, we used downsampling to fix this for the LSTM (our first-draft of a final model). We resampled reviews for the training and validation data, restricting our downsampled data to contain 37,000 reviews from each star-class for training and around 9,000 for the validation data. This is because 2-star reviews were the least frequent class of reviews in our histograms and there were about 50,000 of them before doing the train-test split. We did not perform downsampling on the test data so that we can evaluate the model on data that accurately reflects real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5609ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d_train = []\n",
    "y_d_train = []\n",
    "\n",
    "X_d_val = []\n",
    "y_d_val = []\n",
    "\n",
    "_, train_counts = np.unique(y_train, return_counts = True)\n",
    "smallest_train = min(train_counts)\n",
    "_, val_counts = np.unique(y_val, return_counts = True)\n",
    "smallest_val = min(val_counts)\n",
    "\n",
    "for i in range(5):\n",
    "    X_di_train, y_di_train = resample(X_train[y_train == i], y_train[y_train == i], \n",
    "                                      random_state = 109, \n",
    "                                      n_samples = smallest_train, \n",
    "                                      replace = False)\n",
    "    X_di_val, y_di_val = resample(X_val[y_val == i], y_val[y_val == i], \n",
    "                                  random_state = 109,\n",
    "                                  n_samples = smallest_val,\n",
    "                                  replace = False)\n",
    "    X_d_train.append(X_di_train)\n",
    "    y_d_train.append(y_di_train)\n",
    "    \n",
    "    X_d_val.append(X_di_val)\n",
    "    y_d_val.append(y_di_val)\n",
    "    \n",
    "X_d_train = np.stack(X_d_train).reshape(smallest_train * 5, X.shape[1])\n",
    "y_d_train = np.stack(y_d_train).flatten()\n",
    "\n",
    "X_d_val = np.stack(X_d_val).reshape(smallest_val * 5, X.shape[1])\n",
    "y_d_val = np.stack(y_d_val).flatten()\n",
    "\n",
    "X_d_train.shape, y_d_train.shape, X_d_val.shape, y_d_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee32190",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_d_val, bins=np.linspace(-0.5, 4.5, 6), rwidth=0.7)\n",
    "plt.xticks(range(5), range(1, 6))\n",
    "plt.xlabel('Star Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Star Ratings in Downsampled Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_data = tf.data.Dataset.from_tensor_slices((X_d_train, y_d_train)).shuffle(len(X_d_train)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "d_val_data = tf.data.Dataset.from_tensor_slices((X_d_val, y_d_val)).shuffle(len(X_d_val)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b869e494",
   "metadata": {},
   "source": [
    "### Training FFNN on Downsampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e018ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff2 = tf.keras.models.clone_model(ff1)\n",
    "ff2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "ff2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff2.fit(d_train_data, validation_data=d_val_data, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7dd0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(ff2.predict(X_test, batch_size = 1024), axis = -1).flatten() \n",
    "\n",
    "# Creating a confusion matrix, which compares the y_test and y_pred\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "cm_df = pd.DataFrame(cm, index = [1, 2, 3, 4, 5], columns = [1, 2, 3, 4, 5]) \n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(5,4)) \n",
    "sns.heatmap(cm_df, annot=True) \n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values') \n",
    "plt.xlabel('Predicted Values') \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8824a71",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "For our final model, we decided to use LSTM. Given that our naive model performed poorly, even after downsampling, we decided that a language model was necessary in order to get the level of test accuracy we desired (at least >75%). In the initial draft of our final model, we were able to achieve training accuracy of 75% and testing accuracy of 60% on our dataset after around 10 epochs. However, we found that our model suffered from overfitting (after around 5 epochs, the validation loss began to rise quickly). \n",
    "\n",
    "In future iterations of this model, we hope to add ELMo embeddings. This is better than the standard Keras Embedding layer (word2vec) that we used in first training the LSTM model because ELMo works bidirectionally (considering semantic contexts from before and after a word in a sentence) whereas word2vec only works in one direction. Further, ELMo embeddings are created on a character-basis–not on a word basis–so that our model can learn contexts of words it hasn’t yet encountered. This could be useful in our context because all of the data has been generated by people, prone to errors like spelling mistakes. The ELMo embeddings will let us reflect contexts of mis-spelled/erroneous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(X_d_train.shape[1],))\n",
    "n = Embedding(len(word2idx) + 1, 32, mask_zero=True)(inputs)\n",
    "n = LSTM(32)(n)\n",
    "outputs = Dense(5, activation='softmax')(n)\n",
    "model2 = Model(inputs=inputs, outputs=outputs, name=\"LSTM\")\n",
    "model2.summary()\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(train_data, validation_data=val_data, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 11), model2.history.history['loss'], label='train')\n",
    "plt.plot(range(1, 11), model2.history.history['val_loss'], label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Categorical Cross Entropy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ea8b1e7-ce8e-4c81-a178-54ed087ec4d2",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "In order to test this model on new input, we created a simple function which tokenized and padded sample reviews. It seems that our model is fairly close to predicting stars in the same way we would (when we use `argmax` to find the class with the highest probability). In particular, the model is good at predicting the extreme negativity expressed in the final review we tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_input(string):\n",
    "    return pad_sequences(np.array([[word2idx[w] for w in wordpunct_tokenize(string.lower())]]), X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.predict(as_input('This food is so bad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b79a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.predict(as_input('This food is so good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d07866",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.predict(as_input('One of the worst places Ive been, unfortunately! Rude staff, horrible service and tasteless food and coffee. Would never come back or recommend this place.'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18a40464",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Hugging Face Model\n",
    "\n",
    "Based on the work we did in the most recent lab, we would also like to expand our project to include a sentiment analysis. To classify the sentiment of the reviews, we will use a pipeline from Hugging Face. The code below shows how the sentiment analysis pipeline performs on the first five reviews in our dataset, and it seems to be fairly accurate even without any fine tuning.\n",
    "\n",
    "As noted in our previous milestone, we thought that we could map four- and five-star reviews into a “positive” category, while one- through three-star reviews would be “negative.” We plan to compare sentiments to users’ star reviews by creating a stacked bar chart: Each bar will represent a number of stars, and the stacks will be based on the number of positive and negative reviews in each star category. This should show us whether users deemed to have written negative reviews actually assign one-three stars to those businesses, or if they are perhaps too sympathetic and leave four-five stars despite writing bad reviews. We expect the results of the three-star bar to be most interesting because it’s plausible that some users associate three stars with positivity/neutrality while others associate it with negativity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/review1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad30c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  print(df['text'][i])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to truncate data to be able to predict on all reviews\n",
    "cls(list(df['text'][0:5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
